{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed24be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "INSTALL = False # Switch this to install dependencies\n",
    "if INSTALL: # Try installing package with extras\n",
    "    REPO_URL = \"https://github.com/facebookresearch/dinov2\"\n",
    "    !{sys.executable} -m pip install -e {REPO_URL}'[extras]' --extra-index-url https://download.pytorch.org/whl/cu117  --extra-index-url https://pypi.nvidia.com\n",
    "else:\n",
    "    REPO_PATH = \"/home/osero/Desktop/CMPE/dinov2\" # Specify a local path to the repository (or use installed package instead)\n",
    "    sys.path.append(REPO_PATH)\n",
    "\n",
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple notebook demonstrating how to extract an attention map from DinoV2 inference (with registers) \n",
    "\n",
    "# Most of the core code was originally published here:\n",
    "#  https://gitlab.com/ziegleto-machine-learning/dino/-/tree/main/\n",
    "\n",
    "# November 11th, 2023 by Lance Legel (lance@3co.ai) from 3co, Inc. (https://3co.ai)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e642af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large\n",
    "from matplotlib.colors import Normalize\n",
    "from io import BytesIO\n",
    "import requests\n",
    "os.environ[\"XFORMERS_DISABLED\"] = \"1\" # Switch to enable xFormers\n",
    "print(os.environ.get(\"XFORMERS_DISABLED\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are settings for ensuring input images to DinoV2 are properly sized\n",
    "\n",
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(img)\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "image_dimension = 952\n",
    "    \n",
    "# This is what DinoV2 sees\n",
    "target_size = (image_dimension, image_dimension)\n",
    "\n",
    "# During inference / testing / deployment, we want to remove data augmentations from the input transform:\n",
    "data_transforms = transforms.Compose([ ResizeAndPad(target_size, 14),\n",
    "                                       transforms.CenterCrop(image_dimension),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                     ]\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e741d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (image_dimension, image_dimension)\n",
    "output_dir = '.'\n",
    "patch_size = 14\n",
    "n_register_tokens = 4\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#model.load_state_dict(torch.load('/home/osero/Desktop/CMPE/dinov2/my_test/dinov2_vitg14_pretrain.pth'))\n",
    "model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\").to(device)\n",
    "\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of the image\n",
    "# image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/STS-124_launch_from_a_distance.jpg/800px-STS-124_launch_from_a_distance.jpg\"  # Replace with your image URL\n",
    "\n",
    "# # Download the image\n",
    "# response = requests.get(image_url)\n",
    "\n",
    "# # Check if the request was successful\n",
    "# if response.status_code == 200:\n",
    "#     # Open the image\n",
    "#     original_image = Image.open(BytesIO(response.content))\n",
    "\n",
    "#     # Display the image\n",
    "#     display(original_image)\n",
    "# else:\n",
    "#     print(f\"Failed to download the image. Status code: {response.status_code}\")\n",
    "\n",
    "original_image = Image.open('dino_test_imgs/STS-124_launch_from_a_distance.jpg')\n",
    "\n",
    "(original_w, original_h) = original_image.size\n",
    "\n",
    "img = data_transforms(original_image)\n",
    "\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "img = img[:, :w, :h]\n",
    "\n",
    "w_featmap = img.shape[-2] // patch_size\n",
    "h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = model.get_last_self_attention(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c811fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attention {}: {}\".format(attention.shape, attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46448cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_heads = attention.shape[1]\n",
    "\n",
    "# attention tokens are packed in after the first token; the spatial tokens follow\n",
    "attention = attention[0, :, 0, 1 + n_register_tokens:].reshape(number_of_heads, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1dc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution of attention from transformer tokens\n",
    "attention = attention.reshape(number_of_heads, w_featmap, h_featmap)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b647bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale to higher resolution closer to original image\n",
    "attention = nn.functional.interpolate(attention.unsqueeze(0), scale_factor=patch_size, mode = \"nearest\")[0].cpu()\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all attention across the 12 different heads, to get one map of attention across entire image\n",
    "attention = torch.sum(attention, dim=0)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718826b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate attention map back into original image dimensions\n",
    "attention_of_image = nn.functional.interpolate(attention.unsqueeze(0).unsqueeze(0), size=(original_h, original_w), mode='bilinear', align_corners=False)\n",
    "attention_of_image = attention_of_image.squeeze()\n",
    "print(attention_of_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac79f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image_metric to the range [0, 1]\n",
    "image_metric = attention_of_image.numpy()\n",
    "normalized_metric = Normalize(vmin=image_metric.min(), vmax=image_metric.max())(image_metric)\n",
    "\n",
    "# Apply the Reds colormap\n",
    "reds = plt.cm.Reds(normalized_metric)\n",
    "\n",
    "# Create the alpha channel\n",
    "alpha_max_value = 1.00  # Set your max alpha value\n",
    "\n",
    "# Adjust this value as needed to enhance lower values visibility\n",
    "gamma = 0.5  \n",
    "\n",
    "# Apply gamma transformation to enhance lower values\n",
    "enhanced_metric = np.power(normalized_metric, gamma)\n",
    "\n",
    "# Create the alpha channel with enhanced visibility for lower values\n",
    "alpha_channel = enhanced_metric * alpha_max_value\n",
    "\n",
    "# Add the alpha channel to the RGB data\n",
    "rgba_mask = np.zeros((image_metric.shape[0], image_metric.shape[1], 4))\n",
    "rgba_mask[..., :3] = reds[..., :3]  # RGB\n",
    "rgba_mask[..., 3] = alpha_channel  # Alpha\n",
    "\n",
    "# Convert the numpy array to PIL Image\n",
    "rgba_image = Image.fromarray((rgba_mask * 255).astype(np.uint8))\n",
    "\n",
    "# Save the image\n",
    "rgba_image.save('attention_mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bc06b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the attention mask with PIL\n",
    "attention_mask_image = Image.open(\"{}/attention_mask.png\".format(output_dir))\n",
    "\n",
    "# Ensure both images are in the same mode\n",
    "if original_image.mode != 'RGBA':\n",
    "    original_image = original_image.convert('RGBA')\n",
    "\n",
    "# Overlay the second image onto the first image\n",
    "# The second image must be the same size as the first image\n",
    "original_image.paste(attention_mask_image, (0, 0), attention_mask_image)\n",
    "\n",
    "# Save or show the combined image\n",
    "original_image.save('image_with_attention.png')\n",
    "\n",
    "# Or display it\n",
    "display(original_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
